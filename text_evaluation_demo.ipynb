{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiftyOne Text Evaluation Metrics Plugin\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harpreetsahota204/text_evaluation_metrics/blob/main/text_evaluation_demo.ipynb)\n",
    "\n",
    "This notebook demonstrates the **Text Evaluation Metrics** plugin for FiftyOne.\n",
    "\n",
    "## Available Metrics\n",
    "\n",
    "1. **ANLS** - Average Normalized Levenshtein Similarity (primary VLM OCR metric)\n",
    "2. **Exact Match** - Binary exact match accuracy\n",
    "3. **Normalized Similarity** - Continuous similarity without threshold\n",
    "4. **CER** - Character Error Rate\n",
    "5. **WER** - Word Error Rate\n",
    "\n",
    "üîó [GitHub Repository](https://github.com/harpreetsahota204/text_evaluation_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install FiftyOne and the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q fiftyone python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Plugin\n",
    "\n",
    "Download and install the plugin directly from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/text_evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import libraries and check versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.operators as foo\n",
    "\n",
    "print(f\"FiftyOne version: {fo.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Compute ANLS\n",
    "\n",
    "**ANLS** is the primary metric for VLM OCR evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anls_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_anls\")\n",
    "\n",
    "result = anls_op(dataset, pred_field=\"prediction\", gt_field=\"ground_truth\", threshold=0.5)\n",
    "\n",
    "print(f\"Mean ANLS: {result['mean_anls']:.3f}\")\n",
    "print(\"\\nPer-Sample ANLS:\")\n",
    "for s in dataset:\n",
    "    print(f\"{s['description']:20s} | {s['prediction_anls']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Compute Exact Match\n",
    "\n",
    "**Exact Match** returns 1.0 only for perfect matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_exact_match\")\n",
    "\n",
    "result = em_op(dataset, pred_field=\"prediction\", gt_field=\"ground_truth\")\n",
    "\n",
    "print(f\"Accuracy: {result['accuracy']:.1%}\")\n",
    "print(\"\\nPer-Sample Exact Match:\")\n",
    "for s in dataset:\n",
    "    match = \"‚úì\" if s['prediction_exact_match'] == 1.0 else \"‚úó\"\n",
    "    print(f\"{s['description']:20s} | {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Compute Normalized Similarity\n",
    "\n",
    "**Normalized Similarity** provides continuous scores without threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_normalized_similarity\")\n",
    "\n",
    "result = sim_op(dataset, pred_field=\"prediction\", gt_field=\"ground_truth\")\n",
    "\n",
    "print(f\"Mean Similarity: {result['mean_similarity']:.3f}\")\n",
    "print(\"\\nPer-Sample Similarity (sorted):\")\n",
    "for s in sorted(dataset, key=lambda x: x['prediction_similarity'], reverse=True):\n",
    "    bar = \"‚ñà\" * int(s['prediction_similarity'] * 30)\n",
    "    print(f\"{s['description']:20s} | {s['prediction_similarity']:.3f} | {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Compute CER (Character Error Rate)\n",
    "\n",
    "**CER** measures character-level edits needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_cer\")\n",
    "\n",
    "result = cer_op(dataset, pred_field=\"prediction\", gt_field=\"ground_truth\")\n",
    "\n",
    "print(f\"Mean CER: {result['mean_cer']:.3f} (lower is better)\")\n",
    "print(\"\\nPer-Sample CER (sorted):\")\n",
    "for s in sorted(dataset, key=lambda x: x['prediction_cer']):\n",
    "    quality = \"Excellent\" if s['prediction_cer'] < 0.1 else \"Good\" if s['prediction_cer'] < 0.2 else \"Fair\" if s['prediction_cer'] < 0.5 else \"Poor\"\n",
    "    print(f\"{s['description']:20s} | {s['prediction_cer']:.3f} | {quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Compute WER (Word Error Rate)\n",
    "\n",
    "**WER** measures word-level edits needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_op = foo.get_operator(\"@harpreetsahota/text-evaluation-metrics/compute_wer\")\n",
    "\n",
    "result = wer_op(dataset, pred_field=\"prediction\", gt_field=\"ground_truth\")\n",
    "\n",
    "print(f\"Mean WER: {result['mean_wer']:.3f} (lower is better)\")\n",
    "print(\"\\nPer-Sample WER (sorted):\")\n",
    "for s in sorted(dataset, key=lambda x: x['prediction_wer']):\n",
    "    quality = \"Excellent\" if s['prediction_wer'] == 0.0 else \"Good\" if s['prediction_wer'] < 0.3 else \"Fair\" if s['prediction_wer'] < 0.6 else \"Poor\"\n",
    "    print(f\"{s['description']:20s} | {s['prediction_wer']:.3f} | {quality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Comprehensive Comparison\n",
    "\n",
    "Compare all metrics side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"COMPREHENSIVE METRIC COMPARISON\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"{'Description':20s} | {'ANLS':>6s} | {'Exact':>5s} | {'Sim':>6s} | {'CER':>6s} | {'WER':>6s}\")\n",
    "print(\"-\" * 100)\n",
    "for s in dataset:\n",
    "    print(f\"{s['description']:20s} | {s['prediction_anls']:6.3f} | {s['prediction_exact_match']:5.0f} | {s['prediction_similarity']:6.3f} | {s['prediction_cer']:6.3f} | {s['prediction_wer']:6.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Mean ANLS:       {dataset.mean('prediction_anls'):.3f}\")\n",
    "print(f\"Accuracy:        {dataset.mean('prediction_exact_match'):.1%}\")\n",
    "print(f\"Mean Similarity: {dataset.mean('prediction_similarity'):.3f}\")\n",
    "print(f\"Mean CER:        {dataset.mean('prediction_cer'):.3f}\")\n",
    "print(f\"Mean WER:        {dataset.mean('prediction_wer'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated all 5 text evaluation metrics.\n",
    "\n",
    "### Key Takeaways\n",
    "- **ANLS** is the primary metric for VLM OCR tasks\n",
    "- **Exact Match** provides a strict accuracy baseline\n",
    "- **Normalized Similarity** helps understand error distribution\n",
    "- **CER/WER** provide detailed error analysis\n",
    "\n",
    "### Resources\n",
    "- üìö [Plugin Documentation](https://github.com/harpreetsahota204/text_evaluation_metrics)\n",
    "- üåê [FiftyOne Docs](https://docs.voxel51.com/)\n",
    "\n",
    "**Author:** Harpreet Sahota | **License:** Apache 2.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
